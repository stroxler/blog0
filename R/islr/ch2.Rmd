---
title: "Introduction to Statistical Learning 2: functions, plots, and the bias-variance tradeoff"
author: "Steven Troxler"
date: "December 19, 2015"
output: html_document
---

```{r echo=FALSE, message=FALSE}
# These options make it so that:
#   (a) output blocks are mixed with code, and prefixed with a comment
#   (b) figures are sized, by default, to use most of the html witdth
#       (you can reset fig.width and fig.height inside the {}'s at the
#       top of any code block, if you want a particular plot to be bigger
#       or smaller)
knitr::opts_chunk$set(
  collapse = TRUE, comment = '#>',
  fig.width = 7, fig.height = 6
)
library(ggplot2)
library(dplyr)
```

# Introduction

Welcome to the second installment of my series going through *An Introduction to Statistical Learning with Applications in R* (ISLR). [Last post](http://www.steventroxler.com/blog/?p=12), we went through some plots of the data to follow along with Chapter 1.

In this post I follow along with Chapter 2 of ISLR. In the lab of that chapter, the authors show how to create `R` functions and plot functions of one and two variables. We go through this material. We then conclude with an example R program which investigates the bias-variance tradeoff, which is a major topic in Chapter 2, using a simulation study.

## R functions

Functions in R look like this:
```r
f <- function(x) {
  ... do some stuff with x to get y ...
  y
}
```

The return value of a function is the value of its last expression. You can use the `return` if you want to break out of the function early, but it isn't needed otherwise.

In preparation for our work on plotting, let's write a function that computes the sum of squares for a point $(x, y) \in \mathbb R ^2$:
```{r}
parabaloid <- function(x, y) {
 x ** 2 + y ** 2
}
```

We can check that it works:
```{r}
parabaloid(1, 1)
parabaloid(2, 3)
```


## Plotting functions in R

In the last blog post, we saw how to plot data using `ggplot2.` In this post, we'll look at how to plot 1d and 2d functions, which is useful for machine learning because we might want to look at loss functions or debug our code with visual help.

### Plotting 1d functions

To plot a 1d function, you can use the simple `plot` function. By default, `plot` makes a scatterplot, but we can draw a curve by using `type = 'l'`:
```{r}
x = 1:100 / 100
plot(x, sqrt(x), type='l')
```

To plot multiple 1d functions, we need to know two more things. First, we can use the `col` argument to add color to a plot. Second, every call to `plot` redraws the window, so if we want to add points or curves to a plot window we already made, we should use the `lines` or `points` functions:
```{r}
plot(x, sqrt(x), type='l', col='blue')
lines(x, x ** (1/3), col='green')
points(x, 0.1 * rnorm(length(x)) + 0.5, col='red')
```

### Plotting 2d functions

The Chapter 2 R lab also introduces how to plot functions of both `x` and `y` in the plane. There are three main approaches: perspective plots which try to show the 3d structure, contour plots, and image plots which use color to indicate the function value.
```{r}
x <- seq(-1, 1, .01)
y <- seq(-1, 1, .01)
feval <- outer(x, y, parabaloid)
```
Above, we used the built-in `outer` function, which - when evaluated with no arguments - forms an outer product `outer(x, y) == x %o% y`, but when given a bivariate function instead evaluates that function on the product space.

To get an image plot, use the `image` function:
```{r}
image(x, y, feval)
```

This type of plot is often used in genetic analyses.

To get a contour plot, use the same call with `contour`:
```{r}
contour(x, y, feval)
```

Finally, to get a pseudo-3d plot, use the `persp` function. The perspective plot plots a grid, so you don't usually want to use quite as many points. Hence here we take "strides" (a term borrowed from `python`) to pick out a smaller set of points than we used in the image and contour plots:
```{r}
strides <- seq(1, length(x), 5)
xstrides <- x[strides]
ystrides <- y[strides]
fstrides <- feval[strides, strides]
persp(xstrides, ystrides, fstrides)
```

## The bias-variance tradeoff: a simulation study of polynomial regression

### Least-squares in two paragraphs

In a regression problem, we often wish to find $E[y|x] = f(x)$, where $f$ is unknown and we have samples $x_n, y_n$ for $n=1,...,N$. In linear regression, we simplify the problem by saying that - for vectors $x_n,$ the expected value of $y_n$ takes the form $f(x_n) = x_n^T \theta$ for a vector $\theta$ of *coefficients*.

A common solution to this problem - we'll cover how to solve it using R packages in Chapter 3 - is to use the *least squares* estimator. This is the value of $\theta$ for which the sum of squared errors, $\sum_n (y_n - x_n^T\theta)^2,$ is as small as possible. If we line up the $x_n$ as rows of a matrix $X$, it can be shown that the least squares estimate of $\theta$ - which we call $\hat \theta$ - has the form
$$
\hat\theta = (X^TX)^{-1}X^T y,
$$
where $y$ is the column vector of the $y_n$.

### Least squares polynomial regression

In a *least squares polynomial regression* problem, we have observations $x_n, y_n,$ and we hypothesize that the expected value $E[y|x]$ takes the form 
$$
E[y|x] = \theta_0 + \theta_1 x + \theta_2 x^2 + ... + \theta_p x^p.
$$

If we want to estimate this problem using least-squares, we can cast it as a regular linear regression. Take the initial observations $x_n$, and form from them row vectors
$$
\tilde x_n = (1, x_n, x_n^2, ... x_n^p)
$$
Our nonlinear, polynomial regression in the $x_n$ are actually linear in the $\tilde x_n$, so if we stack up these row vectors in a matrix $X$, once again $\hat \theta = (X^T X)^{-1}X^T y.$

### Code for polynomial regression

Let's write our polynomial regression function next. It should take in a vector `x` of covariates, a vector `y` of observations, and a polynomial degree `p`. It should spit out a least squares estimate `thetahat` of the best polynomial fit.

Before we write the full polynomial regression function, let's make a building block: the function which converts a vector `x` of covariates into a full matrix `X` with the polynomial terms:
```{r}
covariates_to_matrix <- function(x, p) {
  X = matrix(0., nrow=length(x), ncol=p+1)
  for (p_ in 0:p) {
    X[,p_+1] = x ** (p_)
  }
  X
}
```

Note that we didn't build up `X` using `cbind`. Doing so is a common R idiom, but `cbind` creates a whole new array by copying data each time, so if we have a lot of data and `p` is large, the copies can be very expensive.

Now we can write the polynomial regression function
```{r}
polynomial_least_squares <- function(x, y, p) {
  X = covariates_to_matrix(x, p)
  thetahat = solve(t(X) %*% X, t(X) %*% y)
  thetahat
}
```

And of course, to be useful our model needs a way to make predictions on new data:
```{r}
polynomial_prediction <- function(new_x, thetahat) {
  p = length(thetahat) - 1
  X = covariates_to_matrix(new_x, p)
  X %*% thetahat
}
```

### Generating data

In real-life machine learning applications, we don't know the true distribution of the data. The model we wind up estimating is almost always a simplification. To create an analogous situation with simulated data in our polynomial regression, we'll use a cosine curve as the true function: $f(x) = \cos(x)$.


Let's start with an R function to generate our $x_n$s. We'll just use standard normal samples:
```{r}
generate_xs <- function(N) {
  rnorm(N)
}
```

Given the $xs$ we can generate random $y$s. We'll use a standard deviation of 0.25 so that the curve will be obvious in plots:
```{r}
generate_ys <- function(xs) {
  cos(xs) + 0.25 * rnorm(length(xs))
}
```

We can make a new function to generate both the $x_n$ and $y_n$ by wrapping these together in an R `list`
```{r}
generate_xys <- function(N) {
  xs <- generate_xs(N)
  ys <- generate_ys(xs)
  list(xs = xs, ys = ys)
}
```

Before we actually run these functions, it's a good idea to set the state of R's random number generator, so that our results will be repeatable
```{r}
set.seed(42)
```


### A function to look at the bias-variance tradeoff via simulation

The code for performing this simulation is longer than most of the functions we've written before. We could break it up into well-named parts given time -- and we should, if this were a production codebase that needed maintenance. But to get a flavor of what R code looks like before it's been polished, let's just take a look at one big function for producing bias and variance estimates in our simulation problem:
```{r}
compute_bias_and_variance <- function(N, min_p, max_p, ntrial) {
  # generate test data
  n_test = 1000
  test_x = generate_xs(N = n_test)
  e_test_y = cos(test_x)
  
  # pre-make the matrix for fitted values. This saves
  # the computer from needing to allocate / garbage collect it
  # over and over again.
  fit_to_test_x = matrix(0., nrow = ntrial, ncol = n_test)
  
  # make vectors to hold our bias and variance estimates for
  # each p
  bias2_per_p = numeric(max_p - min_p + 1)
  variance_per_p = numeric(max_p - min_p + 1)
  
  # for each p...
  for (p in min_p:max_p) {
    # make a test_X matrix by expanding to the polynomial
    # covariates
    test_X = covariates_to_matrix(test_x, p)
    
    # for each trial...
    for (trial in 1:ntrial) {
      # generate x and y, estimate thetahat, and save the fitted
      # values relative to test_x
      data = generate_xys(N)
      thetahat = polynomial_least_squares(data$xs, data$ys, p)
      fit_to_test_x[trial,] = test_X %*% thetahat
    }
    
    # now that the trials are all finished, compute the
    # average fitted value for each entry of test_x across all
    # trials, and the variance across all trials
    pointwise_mean = colMeans(fit_to_test_x)
    pointwise_var = apply(fit_to_test_x, 2, var)
    
    # our estimates of the bias and variance are averages across
    # all the test_x values. We use `bias2` for the bias since
    # we are actually computing the square bias.
    bias2 = mean((pointwise_mean - e_test_y) ** 2)
    variance = mean(pointwise_var)
    
    # we need to save the bias and variance estimates
    bias2_per_p[p - min_p + 1] = bias2
    variance_per_p[p - min_p + 1] = variance
  }
  list(ps = min_p:max_p,
       bias2 = bias2_per_p,
       variance = variance_per_p)
}
```
This function is long and ugly, but if you dive into statistics and machine learning problems, you'll see a lot of code like it. I've tried to comment it well - always a good idea if you want to share your code or remember what it does in six months.

I'd like to point out one line that might be particularly confusing: `pointwise_var = apply(fit_to_test_x, 2, var)`. This line says to take `fit_to_test_x` and, for each column (the `2`), take that column and pass it to the `var` function. The output is a 1d vector with the variance of each column.

### Looking at the output of our simulation

Let's run the simulation for polynomials of degree up to 8:
```{r}
bv = compute_bias_and_variance(60, 1, 6, 7000)
```

We want to produce a plot that decomposes the MSE of our estimation into bias and variance. The MSE is the sum of the bias and variance, so we can get it from `bv`. We plot the bias in blue, the variance in orange, and the MSE in green.
```{r}
plot(bv$ps, bv$bias2, type='l', col='blue', xlab='p', ylab='squared error')
lines(bv$ps, bv$variance, col='orange')
lines(bv$ps, bv$bias2 + bv$variance, col='green')
```

Here we can clearly see the U-shaped error, where the bias drops toward zero as our model becomes more complex, but the variance increases. The best tradeoff looks like it happens around $p=2$. Note that this answer depends on `N = 50`. For larger values of `N`, if you play around with this simulation, you'll see that the best choice of `p` increases.

### Next post

Chapters 1 and 2 were the introductory chapters of ISLR. Next post, we'll start going through how to use R packages to work with different statistical learning models, starting with linear regression.