---
title: 'An Introduction to Statistical Learning, Part 4: Logistic Regression'
author: "Steven Troxler"
date: "December 21, 2015"
output: html_document
---

Welcome to the sixth post in my series on [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/). In the [last post](FIX THISSSS http://www.steventroxler.com/blog/?p=48), we learned how to use `MASS` package's `lda` function for performing linear discriminant analysis. We'll use the same `Smarket` dataset in this tutorial, but use the `class` package's `knn` function for classification.
```{r echo=FALSE, message=FALSE}
# These options make it so that:
#   (a) output blocks are mixed with code, and prefixed with a comment
#   (b) figures are sized, by default, to use most of the html witdth
#       (you can reset fig.width and fig.height inside the {}'s at the
#       top of any code block, if you want a particular plot to be bigger
#       or smaller)
knitr::opts_chunk$set(
  collapse = TRUE, comment = '#>',
  fig.width = 7, fig.height = 6
)
library(ggplot2)
library(dplyr)
```

## K-Nearest Neighbors

What is K-nearest-neighbors, or KNN? It's a method where we have some notion of the distance between points $x$. When we want to pick a good guess $y$ for the value of $Y$ at some point $x_0$, we take our sample and we find the $k$ points nearest to $x_0$. We then pick the most common $Y$ value associated with those points.

It's one of the most flexible and powerful classification (and regression: instead of picking the most common $Y$ as in classification, we can just average them when $Y$ is a numeric value). With enough data, it can uncover the true value of any function. It's one of the most commonly used algorithms for some types of problems, and also very easy to understand and explain (which can be very important in a business setting, where explaining a model to non-statisticians might be essential).

There are disadvantages to KNN. If we don't have a clever way of organizing our data, it can be expensive to search through the whole dataset when predicting a new value. It is also so flexible that it is very much subject to the curse of dimensionality, and doesn't usually work well when there are more than a few features in $X$.


## Using the `class` library's `knn` function

We'll explore the KNN algorithm in `R` by once again looking at the `Smarket` dataset. Unlike most of the tools we've used for classification up till now, the `knn` function does *not* spit out a model which we can use for prediction. Instead, we hand it the training and test data at the same time.

Although this seems quite different, it makes sense: unlike the other models we've worked with, the `knn` function does not find a small set of parameters from which to make predictions. Instead, it uses all the data to predict new points. So it wouldn't be very effective to spit out a `model` object like `glm` does: this object would have to copy all the data!

It also expects a matrix of data rather than a `data.frame`. This is unusual for `R`, but the different libraries often have different interfaces. To find out how to use a new function, remember you can type `?func` in the R console to bring up its documentation.

Once we've made our predictions, we can look at them using the same `table` and `mean` calculations as before to see the confusion matrix and accuracy.
```{r}
library(class)
smarket <- tbl_df(ISLR::Smarket)
train_df <- smarket %>% filter(Year < 2005)
test_df <- smarket %>% filter(Year >= 2005)
predictions <- knn(
  cbind(train_df$Lag1, train_df$Lag2),
  cbind(test_df$Lag1, test_df$Lag2),
  train_df$Direction, k=1)
table(predictions, test_df$Direction)
mean(predictions == test_df$Direction)
```

This result is disappointing. We got exactly half the test cases right. What would happen if we bumped `k` to a bigger value. Soon we'll see better ways of doing this, using cross-validation (because if we just try out `k` until it works, we are fitting the hyperparameter `k` to the test data, and our results may not be trustworthy), but for now let's just try it out. Since stock data is very noisy, we'll use a pretty big value of `k`, 200:
```{r}
predictions <- knn(
  cbind(train_df$Lag1, train_df$Lag2),
  cbind(test_df$Lag1, test_df$Lag2),
  train_df$Direction, k=200)
table(predictions, test_df$Direction)
mean(predictions == test_df$Direction)
```

Here we see that for `k = 100`, KNN is doing about as well as logistic regression and LDA did: we correctly guess the direction of around 55% of the market moves, using the previous two days' returns as our covariates.

## Next Post

I hope you've enjoyed this quick demonstration of KNN, which is the last classification algorithm we'll go through from chapter 4 of ISLR. Next post, we'll start looking at chapter 5, which discusses cross-validation and the bootstrap. These are alternative ways of measuring the accuracy of a model, which can be used instead of the training/test data split we've been using.